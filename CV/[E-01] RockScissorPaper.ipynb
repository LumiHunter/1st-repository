{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4b0fae2",
   "metadata": {},
   "source": [
    "#가위바위보 분류기 : LeNet Model을 통한 이미지 분류기\n",
    "\n",
    "1. 적절한 파라미터를 골라 트레이닝 수행\n",
    "2. 과적합(overfitting)을 방지하기 위한 다양성있는 데이터 수집\n",
    "3. test accuracy 60% 이상 목표"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bbedcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16aa719a",
   "metadata": {},
   "source": [
    "##준비된 학습 데이터의 이미지 사이즈를 28 * 28로 변경하기 \n",
    "\n",
    "- [teachable machine](https://teachablemachine.withgoogle.com/)에서 찍은 내 손 사진 300장\n",
    "- [roboflow](https://public.roboflow.com/classification/rock-paper-scissors/1/download)에서 제공하는 Rock Paper Scissor Dataset(train용 이미지와 validation용 이미지 모두 train에 사용) 2,892장\n",
    "\n",
    "**가위, 바위, 보 각 1,064장 총 3,192장**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4790f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1064  images to be resized.\n",
      "1064  images resized.\n",
      "1064  images to be resized.\n",
      "1064  images resized.\n",
      "1064  images to be resized.\n",
      "1064  images resized.\n"
     ]
    }
   ],
   "source": [
    "def resize_images(img_path):\n",
    "    images=glob.glob(img_path + \"/*.jpg\")  \n",
    "    \n",
    "    print(len(images), \" images to be resized.\")\n",
    "\n",
    "    target_size=(28,28)\n",
    "    for img in images:\n",
    "        old_img=Image.open(img)\n",
    "        new_img=old_img.resize(target_size,Image.ANTIALIAS)\n",
    "        new_img.save(img, \"JPEG\")\n",
    "    \n",
    "    print(len(images), \" images resized.\")\n",
    "\n",
    "    \n",
    "# 가위\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/scissor\"\n",
    "resize_images(image_dir_path)\n",
    "# 바위\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/rock\"\n",
    "resize_images(image_dir_path)\n",
    "# 보\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/paper\"\n",
    "resize_images(image_dir_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27509c6",
   "metadata": {},
   "source": [
    "##학습 데이터에 준비된 이미지 넣기\n",
    "\n",
    "**모델을 훈련 시킬 때 입력값을 0~1 사이 값으로 적절하게 정규화 시키기 위해, 학습 데이터의 픽셀 값이 최대 얼마인지 확인해야 함.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1585b9b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습데이터(x_train)의 이미지 개수는 3192 입니다.\n"
     ]
    }
   ],
   "source": [
    "def load_data(img_path, number_of_data=3192): \n",
    "    img_size=28\n",
    "    color=3\n",
    "\n",
    "    imgs=np.zeros(number_of_data*img_size*img_size*color,dtype=np.int32).reshape(number_of_data,img_size,img_size,color)\n",
    "    labels=np.zeros(number_of_data,dtype=np.int32)\n",
    "\n",
    "    idx=0\n",
    "    for file in glob.iglob(img_path+'/scissor/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    \n",
    "        labels[idx]=0   \n",
    "        idx=idx+1\n",
    "\n",
    "    for file in glob.iglob(img_path+'/rock/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img \n",
    "        labels[idx]=1 \n",
    "        idx=idx+1  \n",
    "    \n",
    "    for file in glob.iglob(img_path+'/paper/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img  \n",
    "        labels[idx]=2  \n",
    "        idx=idx+1\n",
    "        \n",
    "    print(\"학습데이터(x_train)의 이미지 개수는\", idx,\"입니다.\")\n",
    "    return imgs, labels\n",
    "\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper\"\n",
    "(x_train, y_train)=load_data(image_dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a97a24b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습데이터의 픽셀 값 최소값은  0 , 최대값은  255\n"
     ]
    }
   ],
   "source": [
    "print('학습데이터의 픽셀 값 최소값은 ', np.min(x_train), ', 최대값은 ', np.max(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30d6e50a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (3192, 28, 28, 3)\n",
      "y_train shape: (3192,)\n"
     ]
    }
   ],
   "source": [
    "x_train_norm = x_train/255.0    # 학습데이터 정규화\n",
    "\n",
    "print(\"x_train shape: {}\".format(x_train_norm.shape))\n",
    "print(\"y_train shape: {}\".format(y_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fba7e1fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "라벨:  0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_train_norm[1000])\n",
    "print('라벨: ', y_train[1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0645d3b",
   "metadata": {},
   "source": [
    "##딥러닝 네트워크 설계하기\n",
    "- n_channel: 이미지의 특징을 고려하는 수(ex.손가락이 몇개면 '가위'로 분류하나?)\n",
    "- n_dense: 분류할 이미지의 복잡도에 비례하는 수\n",
    "- n_train_epoch: 모델이 학습을 진행할 횟수\n",
    " \n",
    "**MNIST 숫자 손글씨 구분 모델 학습 진행 시 10번째까지 충분히 accuracy가 상승했다는 점과 학습횟수가 너무 높을 시 overfitting이 발생할 수 있음을 고려해 n_train epoch=5 로 파라미터 결정.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12da8633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model에 추가된 Layer 개수:  7\n"
     ]
    }
   ],
   "source": [
    "n_channel_1 = 100\n",
    "n_channel_2 = 100\n",
    "n_dense = 100\n",
    "n_train_epoch = 5\n",
    "\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Conv2D(n_channel_1, (3,3), activation = 'relu', input_shape = (28,28,3)))\n",
    "model.add(keras.layers.MaxPool2D(2,2))\n",
    "model.add(keras.layers.Conv2D(n_channel_2, (3,3), activation = 'relu'))\n",
    "model.add(keras.layers.MaxPooling2D((2,2)))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(n_dense, activation = 'relu'))\n",
    "model.add(keras.layers.Dense(3, activation = 'softmax'))\n",
    "\n",
    "print('Model에 추가된 Layer 개수: ', len(model.layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "45230a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 26, 26, 100)       2800      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 13, 13, 100)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 11, 11, 100)       90100     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 5, 5, 100)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 2500)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 100)               250100    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 343,303\n",
      "Trainable params: 343,303\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c13e211",
   "metadata": {},
   "source": [
    "##딥러닝 네트워크 모델 학습시키기\n",
    "\n",
    "**학습데이터는 (데이터갯수, 이미지 크기 x, 이미지 크기 y, 채널수) 형태로 입력되어야 함(x_train shape: (3192, 28, 28, 3)임을 확인함)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "87152a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "100/100 [==============================] - 1s 4ms/step - loss: 0.5762 - accuracy: 0.7497\n",
      "Epoch 2/5\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.0786 - accuracy: 0.9815\n",
      "Epoch 3/5\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.0237 - accuracy: 0.9966\n",
      "Epoch 4/5\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.0070 - accuracy: 0.9997\n",
      "Epoch 5/5\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.0040 - accuracy: 0.9994\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f67ed2756a0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer = 'adam',\n",
    "             loss = 'sparse_categorical_crossentropy',\n",
    "             metrics = ['accuracy'])\n",
    "model.fit(x_train_norm, y_train, epochs = n_train_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872acc81",
   "metadata": {},
   "source": [
    "##테스트 데이터 불러오기\n",
    "- [roboflow](https://public.roboflow.com/classification/rock-paper-scissors/1/download)에서 제공하는 Rock Paper Scissor Dataset(test) 33장\n",
    "\n",
    "**학습용 데이터와 같은 방법으로 불러오고, 정규화**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dc80fed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11  images to be resized.\n",
      "11  images resized.\n",
      "11  images to be resized.\n",
      "11  images resized.\n",
      "11  images to be resized.\n",
      "11  images resized.\n",
      "테스트데이터(x_test)의 이미지 개수는 33 입니다.\n",
      "테스트데이터의 픽셀 값 최소값은  0 , 최대값은  255\n"
     ]
    }
   ],
   "source": [
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/test/t_scissor\"\n",
    "resize_images(image_dir_path)\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/test/t_rock\"\n",
    "resize_images(image_dir_path)\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/test/t_paper\"\n",
    "resize_images(image_dir_path)\n",
    "\n",
    "def load_data(img_path, number_of_data=33): \n",
    "    img_size=28\n",
    "    color=3\n",
    "\n",
    "    imgs=np.zeros(number_of_data*img_size*img_size*color,dtype=np.int32).reshape(number_of_data,img_size,img_size,color)\n",
    "    labels=np.zeros(number_of_data,dtype=np.int32)\n",
    "\n",
    "    idx=0\n",
    "    for file in glob.iglob(img_path+'/t_scissor/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    \n",
    "        labels[idx]=0   \n",
    "        idx=idx+1\n",
    "\n",
    "    for file in glob.iglob(img_path+'/t_rock/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img \n",
    "        labels[idx]=1 \n",
    "        idx=idx+1  \n",
    "    \n",
    "    for file in glob.iglob(img_path+'/t_paper/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img  \n",
    "        labels[idx]=2  \n",
    "        idx=idx+1\n",
    "        \n",
    "    print(\"테스트데이터(x_test)의 이미지 개수는\", idx,\"입니다.\")\n",
    "    return imgs, labels\n",
    "\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/test\"\n",
    "(x_test, y_test)=load_data(image_dir_path)\n",
    "\n",
    "print('테스트데이터의 픽셀 값 최소값은 ', np.min(x_test), ', 최대값은 ', np.max(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2d2862cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_test shape: (33, 28, 28, 3)\n",
      "y_test shape: (33,)\n"
     ]
    }
   ],
   "source": [
    "x_test_norm = x_test/255.0  \n",
    "\n",
    "print(\"x_test shape: {}\".format(x_test_norm.shape))\n",
    "print(\"y_test shape: {}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ad8277",
   "metadata": {},
   "source": [
    "##테스트 데이터로 성능 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1849c127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 - 0s - loss: 0.0353 - accuracy: 1.0000\n",
      "test_lose: 0.035277657210826874\n",
      "test_accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(x_test_norm, y_test, verbose = 2)\n",
    "print(\"test_lose: {}\".format(test_loss))\n",
    "print(\"test_accuracy: {}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b157e5a4",
   "metadata": {},
   "source": [
    "###회고\n",
    "\n",
    "- 처음 학습 데이터를 만들 때, 내 손 이미지 300장만으로 모델을 학습시킨 탓에 overfitting이 심하게 발생했음(accuracy 6%대...)\n",
    "- 프리소스 이미지를 구한 것은 다행이나, 많은 양의 데이터를 파일 경로에 업로드하는 과정이 너무 느려 직접 업로드하지 않고 zip 파일로 묶은 다음 cloud shell에서 unzip 명령을 반복적으로 내리는 데에서 시간 소요가 컸음.\n",
    "\n",
    "* accuracy를 높이기 위해 시도한 것들: 학습 데이터 양을 늘리고 파라미터(n_channel_1, n_channel_2, n_dense, n_train epoch)를 조절했음.\n",
    "\n",
    "* 알게 된 점: 학습 데이터는 가능한 많이, 다양하게. 특히 이번 손 모양처럼, 사람에 따라 그 모양이 아주 다양할 수 있는 데이터는 한사람이 만든 것으로는 한계가 크다..! 반드시 여러 사람의 샘플이 필요.\n",
    "\n",
    "* 궁금한 점1: 입력 데이터는 분명 이미지 파일인데, 정규화 과정이 단순 최대 픽셀 수 나누기 연산으로 시행되는 원리가 뭘까? \n",
    "\n",
    "* 궁금한 점2: model.compile() 과정은 무슨 뜻일까?\n",
    "\n",
    "* 모호한 점1: n_channel_1, n_channel_2, n_dense의 의미는 구체적으로 어떤 것일까? n_train_epoch는 지나치게 높을 경우 overfitting이 일어난다는 점이 이해되지만 n_channel_1, n_channel_2, n_dense는 비용적인 부분에서 한계가 없다면 늘리면 늘릴 수록 좋은 걸까?\n",
    "\n",
    "* 모호한 점2: 어차피 데이터를 0~1 픽셀의 작은 크기로 정규화할 것이라면 이미지 사이즈를 28 * 28이 아니라 더 작은 크기로 변경했어야 하는 게 아닐까?\n",
    "\n",
    "\n",
    "* 다짐: 학습 데이터는 많이, 다양하게."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
