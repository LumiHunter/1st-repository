# -*- coding: utf-8 -*-
"""[CV-1] ResNet Ablation Study.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LRI3I0eGorzDVyeipArAYDn5q6RxOKb1

## residual connection을 통한 성능 향상

### 학습 목표
- 직접 ResNet 구현하기
- 모델을 config에 따라서 변경 가능하도록 만들기
- 직접 실험해서 성능 비교하기

### 루브릭 
- ResNet-34, ResNet-50 모델 구현이 정상적으로 진행되었는가? : **블록함수 구현이 제대로 진행되었으며 구현한 모델의 summary가 예상된 형태로 출력되었다.**
- 구현한 ResNet 모델을 활용하여 Image Classification 모델 훈련이 가능한가? : **cats_vs_dogs 데이터셋으로 학습시 몇 epoch동안 안정적으로 loss 감소가 진행 확인되었다.**
- Ablation Study 결과가 바른 포맷으로 제출되었는가? : **ResNet-34, ResNet-50 각각 plain모델과 residual모델을 동일한 epoch만큼 학습시켰을 때의 validation accuracy 기준으로 Ablation Study 결과표가 작성되었다.**


## 목차
### 0. 환경 설정
### 1. 데이터셋 준비
#### 1) cats_vs_dogs 데이터 로드
#### 2) Input Normalization
### 2. 모델 구성
#### 1) ResNet-34, ResNet-50
#### 2) PlainNet-34, PlainNet-50
#### 3) 모델 훈련
### 3. Ablation Study
#### 1) training loss, valiation accuracy 비교
#### 2) 결론
---

### 0. 환경 설정
"""

from google.colab import auth
auth.authenticate_user()

from google.colab import drive
drive.mount('/content/gdrive', force_remount=False)

import os
from pathlib import Path

folder = 'Colab Notebooks'
project_dir = 'going_deeper_CV'

base_path = Path('/content/gdrive/My Drive')
project_path = base_path / folder / project_dir
os.chdir(project_path)
for x in list(project_path.glob("*")):
  if x.is_dir():
    dir_name = str(x.relative_to(project_path))
    os.rename(dir_name, dir_name.split(" ", 1)[0])
print(f'현재 디렉토리 위치: {os.getcwd()}')

import tensorflow as tf
from tensorflow import keras

import numpy as np
import matplotlib.pyplot as plt

import tensorflow_datasets as tfds

print(tf.__version__)
print(np.__version__)

tf.config.list_physical_devices('GPU')

"""### 1. 데이터셋 준비

#### 1) cats_vs_dogs 데이터 로드
"""

(ds_train, ds_test), ds_info = tfds.load(
    'cats_vs_dogs', 
    split = ['train[:80%]','train[80%:]'],
    as_supervised=True, 
    shuffle_files=True, 
    with_info=True, 
    data_dir=os.getcwd()+'/datasets'
    )

print(ds_info.features)

print(tf.data.experimental.cardinality(ds_train))
print(tf.data.experimental.cardinality(ds_test))

ds_info.features["label"].num_classes

ds_info.features["label"].names

fig = tfds.show_examples(ds_train, ds_info)

fig = tfds.show_examples(ds_test, ds_info)

"""#### 2) Input Normalization
- scale이 큰 feature의 영향이 비대해지는 것을 방지
- 지역해(Local optimum)에 수렴하는 위험을 줄임(학습 속도 향상)
"""

def normalize_and_resize_img(image, label):
    """Normalizes images: `uint8` -> `float32`."""
    image = tf.image.resize(image, [32, 32]) 
    return tf.cast(image, tf.float32) / 255., label

"""크기가 서로 다른 이미지들에 `tf.image.resize()` 를 적용해 크기를 통일한 다음, 픽셀 정보의 최댓값인 255로 나누어 데이터 범위를 0~1로 제한"""

def apply_normalize_on_dataset(ds, is_test=False, batch_size=16):
    ds = ds.map(
        normalize_and_resize_img, 
        num_parallel_calls=1
    )
    ds = ds.batch(batch_size)
    if not is_test:
        ds = ds.repeat()
        ds = ds.shuffle(200)
    ds = ds.prefetch(tf.data.experimental.AUTOTUNE)
    return ds

"""### 2. 모델 구성

#### 1) ResNet-34, ResNet-50
"""

def residual_block(input_layer, num_res, channel, is_50=False):
  # 입력 레이어
  x = input_layer
  if not is_50:
    # ResNet-34
    for res_num in range(num_res):
      identity = x
      # convolution
      x = keras.layers.Conv2D(
          filters=channel,
          kernel_size=(3,3),
          kernel_initializer='he_normal',
          padding='same'
          )(x)
      x = keras.layers.BatchNormalization()(x)
      x = keras.layers.Activation('relu')(x)

      x = keras.layers.Conv2D(
          filters=channel,
          kernel_size=(3,3),
          kernel_initializer='he_normal',
          padding='same'
          )(x)
      x = keras.layers.BatchNormalization()(x)

      identity_channel = identity.shape.as_list()[-1]
      if identity_channel != channel:
        identity = keras.layers.Conv2D(
            filters=channel,
            kernel_size=(1,1),
            padding='same'
        )(identity)

      # skip-connection
      x = keras.layers.Add()([x,identity])
      x = keras.layers.Activation('relu')(x)

  else:
    # ResNet-50
    for res_num in range(num_res):
      identity = x
      #colvolution
      x = keras.layers.Conv2D(
        filters=channel,
        kernel_size=(1,1),
        kernel_initializer='he_normal',
        padding='same'
        )(x)
      x = keras.layers.BatchNormalization()(x)
      x = keras.layers.Activation('relu')(x)

      x = keras.layers.Conv2D(
        filters=channel,
        kernel_size=(3,3),
        kernel_initializer='he_normal',
        padding='same'
        )(x)
      x = keras.layers.BatchNormalization()(x)
      x = keras.layers.Activation('relu')(x)

      x = keras.layers.Conv2D(
        filters=channel*4,
        kernel_size=(1,1),
        kernel_initializer='he_normal',
        padding='same'
        )(x)
      x = keras.layers.BatchNormalization()(x)

      identity_channel = identity.shape.as_list()[-1]
      if identity_channel != channel*4:
        identity = keras.layers.Conv2D(
            filters=channel*4,
            kernel_size=(1,1),
            padding='same'
        )(identity)

      # skip-connection    
      x = keras.layers.Add()([x,identity])
      x = keras.layers.Activation('relu')(x)

  # MaxPooling
  if identity.shape[1] != 1:
    x = keras.layers.MaxPooling2D(pool_size=(2,2),strides=2)(x)

  return x

def build_resnet(input_shape, num_channel_list, num_classes, is_50=False):

  # 입력 레이어
  input_layer = keras.layers.Input(shape=input_shape)
  output = input_layer
  # convolution 1층
  x = keras.layers.Conv2D(
      filters = 64,
      kernel_size = (2,2),
      strides=2,
      padding = 'valid'
      )(output)
  x = keras.layers.BatchNormalization()(x)
  x = keras.layers.Activation('relu')(x)
  x = keras.layers.MaxPool2D(pool_size=(2,2), strides=2, padding='valid')(x)

  # config list들의 길이만큼 블록을 생성
  for (num_res, channel) in (num_channel_list):
    x = residual_block(
        input_layer=x,
        num_res=num_res,
        channel=channel,
        is_50=is_50
    )

  x = keras.layers.AveragePooling2D(padding='same')(x)
  x = keras.layers.Flatten(name='flatten')(x)
  x = keras.layers.Dense(512, activation='relu')(x)
  x = keras.layers.Dense(num_classes, activation='softmax', name='predictions')(x)

  model = keras.Model(inputs=input_layer, outputs=x)

  return model

resnet_34 = build_resnet(
    input_shape=(32,32,3), 
    num_channel_list=[(3,64),(4,128),(6,256),(3,512)],
    num_classes=2,
    is_50=False)

resnet_34.summary()

resnet_50 = build_resnet(
    input_shape=(32,32,3), 
    num_channel_list=[(3,64),(4,128),(6,256),(3,512)],
    num_classes=2,
    is_50=True)

resnet_50.summary()

"""#### 2) PlainNet-34, PlainNet-50

"""

def plain_block(input_layer, num_res, channel, is_50=False):
  # 입력 레이어
  x = input_layer
  if not is_50:
    # PlainNet-34
    for res_num in range(num_res):
      # convolution
      x = keras.layers.Conv2D(
          filters=channel,
          kernel_size=(3,3),
          kernel_initializer='he_normal',
          padding='same'
          )(x)
      x = keras.layers.BatchNormalization()(x)
      x = keras.layers.Activation('relu')(x)

      x = keras.layers.Conv2D(
          filters=channel,
          kernel_size=(3,3),
          kernel_initializer='he_normal',
          padding='same'
          )(x)
      x = keras.layers.BatchNormalization()(x)

  else:
    # PlainNet-50
    for res_num in range(num_res):
      #colvolution
      x = keras.layers.Conv2D(
        filters=channel,
        kernel_size=(1,1),
        kernel_initializer='he_normal',
        padding='same'
        )(x)
      x = keras.layers.BatchNormalization()(x)
      x = keras.layers.Activation('relu')(x)

      x = keras.layers.Conv2D(
        filters=channel,
        kernel_size=(3,3),
        kernel_initializer='he_normal',
        padding='same'
        )(x)
      x = keras.layers.BatchNormalization()(x)
      x = keras.layers.Activation('relu')(x)

      x = keras.layers.Conv2D(
        filters=channel*4,
        kernel_size=(1,1),
        kernel_initializer='he_normal',
        padding='same'
        )(x)
      x = keras.layers.BatchNormalization()(x)

  # MaxPooling
  if x.shape[1] != 1:
    x = keras.layers.MaxPooling2D(pool_size=(2,2),strides=2)(x)

  return x

def build_plainnet(input_shape, num_channel_list, num_classes, is_50=False):

  # 입력 레이어
  input_layer = keras.layers.Input(shape=input_shape)
  output = input_layer
  # convolution 1층
  x = keras.layers.Conv2D(
      filters = 64,
      kernel_size = (2,2),
      strides=2,
      padding = 'valid'
      )(output)
  x = keras.layers.BatchNormalization()(x)
  x = keras.layers.Activation('relu')(x)
  x = keras.layers.MaxPool2D(pool_size=(2,2), strides=2, padding='valid')(x)

  # config list들의 길이만큼 블록을 생성
  for (num_res, channel) in (num_channel_list):
    x = plain_block(
        input_layer=x,
        num_res=num_res,
        channel=channel,
        is_50=is_50
    )

  x = keras.layers.AveragePooling2D(padding='same')(x)
  x = keras.layers.Flatten(name='flatten')(x)
  x = keras.layers.Dense(512, activation='relu')(x)
  x = keras.layers.Dense(num_classes, activation='softmax', name='predictions')(x)

  model = keras.Model(inputs=input_layer, outputs=x)

  return model

plainnet_34 = build_plainnet(
    input_shape=(32,32,3), 
    num_channel_list=[(3,64),(4,128),(6,256),(3,512)],
    num_classes=2,
    is_50=False)

plainnet_34.summary()

plainnet_50 = build_plainnet(
    input_shape=(32,32,3), 
    num_channel_list=[(3,64),(4,128),(6,256),(3,512)],
    num_classes=2,
    is_50=True)

plainnet_50.summary()

"""#### 3) 모델 훈련"""

BATCH_SIZE = 256
EPOCH = 15

ds_train = apply_normalize_on_dataset(ds_train, batch_size=BATCH_SIZE)
ds_test = apply_normalize_on_dataset(ds_test, batch_size=BATCH_SIZE)

resnet_34.compile(
    loss='sparse_categorical_crossentropy',
    optimizer=tf.keras.optimizers.SGD(lr=0.01, clipnorm=1.),
    metrics=['accuracy'],
)

history_res_34 = resnet_34.fit(
    ds_train,
    steps_per_epoch=int(ds_info.splits['train'].num_examples/BATCH_SIZE),
    validation_steps=int(ds_info.splits['train'].num_examples/BATCH_SIZE),
    epochs=EPOCH,
    validation_data=ds_test,
    verbose=1,
    use_multiprocessing=True,
)

resnet_50.compile(
    loss='sparse_categorical_crossentropy',
    optimizer=tf.keras.optimizers.SGD(lr=0.01, clipnorm=1.),
    metrics=['accuracy'],
)

history_res_50 = resnet_50.fit(
    ds_train,
    steps_per_epoch=int(ds_info.splits['train'].num_examples/BATCH_SIZE),
    validation_steps=int(ds_info.splits['train'].num_examples/BATCH_SIZE),
    epochs=EPOCH,
    validation_data=ds_test,
    verbose=1,
    use_multiprocessing=True,
)

plainnet_34.compile(
    loss='sparse_categorical_crossentropy',
    optimizer=tf.keras.optimizers.SGD(lr=0.01, clipnorm=1.),
    metrics=['accuracy'],
)

history_plain_34 = plainnet_34.fit(
    ds_train,
    steps_per_epoch=int(ds_info.splits['train'].num_examples/BATCH_SIZE),
    validation_steps=int(ds_info.splits['train'].num_examples/BATCH_SIZE),
    epochs=EPOCH,
    validation_data=ds_test,
    verbose=1,
    use_multiprocessing=True,
)

plainnet_50.compile(
    loss='sparse_categorical_crossentropy',
    optimizer=tf.keras.optimizers.SGD(lr=0.01, clipnorm=1.),
    metrics=['accuracy'],
)

history_plain_50 = plainnet_50.fit(
    ds_train,
    steps_per_epoch=int(ds_info.splits['train'].num_examples/BATCH_SIZE),
    validation_steps=int(ds_info.splits['train'].num_examples/BATCH_SIZE),
    epochs=EPOCH,
    validation_data=ds_test,
    verbose=1,
    use_multiprocessing=True,
)

"""### 3. Ablation Study

#### 1) training loss, valiation accuracy 비교
"""

# training loss 비교

plt.plot(history_res_34.history['loss'],'r')
plt.plot(history_res_50.history['loss'],'b')
plt.plot(history_plain_34.history['loss'],'m')
plt.plot(history_plain_50.history['loss'],'c')
plt.title('Model training loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['resnet_34', 'resnet_50', 'plainnet_34', 'plainnet_50'], loc='upper left')
plt.show()

# validation accuracy 비교

plt.plot(history_res_34.history['val_accuracy'], 'r')
plt.plot(history_res_50.history['val_accuracy'], 'b')
plt.plot(history_plain_34.history['val_accuracy'], 'm')
plt.plot(history_plain_50.history['val_accuracy'], 'c')
plt.title('Model validation accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['resnet_34', 'resnet_50', 'plainnet_34', 'plainnet_50'], loc='upper left')
plt.show()

"""#### 2) 결론
- epoch 당 training loss가 줄어드는 정도를 비교해봤을 때, **plainnet보다 resnet이 더 빠르게** 줄어들고 있고, 예상과는 다르게 resnet과 plainnet 모두에서 **50층 모델의 loss 감소가 더뎠다.** 
- validation accuracy를 비교했을 때도, **같은 계층에서 plainnet보다 resnet의 accuracy가 더 높은 것으로** 나타난다.
- resnet-34는 논문과는 달리 네 개의 모델 중 가장 좋은 성능을 보이고 있는데, dataset의 class 수가 2개 뿐인 cats vs dog로 실험을 진행하여 논문과 다른 결과가 나온 것이 아닌지 추측해본다. 
- resnet-50은 training loss가 좀처럼 줄어들지 않은 것에 비하면 accuracy가 높은 편이다. 과적합을 방지하면서 모델의 성능을 높이는 잔차 연결의 효과가 나타나는 것으로 보인다.
- plainnet-34는 resnet-34와 비교했을 때 loss와 validation accuracy가 조금 모자라다.
- plainnet-50의 경우 training loss도 많고 validation accuray도 낮아서, 층이 많이 쌓여있음에도 학습이 제대로 진행되지 않은 것처럼 보인다.    

**최종 validation accuray**    
ResNet-34: 0.64 | PlainNet-34: 0.61   
ResNet-50: 0.60 | PlainNet-50: 0.50

## 회고
- block 설계에서 계층 진행에 따라 shape가 달라지는 부분을 잘 조절하지 못하고 시간을 많이 날렸다... convolution layer 또는 pooling layer를 지나면 어떤 식으로 shape가 변하는지 충분히 알고 있다고 생각했는데 막상 설계에서 떠올리는 건 다른 문제인 듯 싶다.
- ResNet은 convolution block과 identity block의 skip-connection이 핵심 아이디어인데, resnet block을 만드는 코드에서 반복되는 내용이 지나치게 길다는 느낌을 받아서, convolution block과 identity block을 따로 만들고, 그들을 조합하는 방식으로도 다시 설계해보는 편이 좋겠다는 생각이 들었다.
- 시간이 없어서(ㅠㅠ) 비교 실험을 해보지 못했는데, dataset을 class 수가 10개 짜리인 cifar-10을 쓰면 논문에서처럼 ResNet-50이 가장 좋은 성능을 보이는 것으로 나타날 지 궁금하다. 그렇다면 분류해야 하는 class 수에 따라 적합한 모델의 계층 수가 다르다는 얘기가 될 테니...
"""

